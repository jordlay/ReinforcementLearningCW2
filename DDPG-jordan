### Importing all the required python modules ###
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim
import numpy as np
import gym
from torch.autograd import Variable
import copy
import time

### Setting hyperparameters ###
# Number of episodes
M = 1000

# Optimizer learning rates #
critic_learning_rate = 0.001
actor_learning_rate = 0.0001

# Magnitude of action noise #
noise = 0.05

# Size of batch #
batch_size = 5

# Discount for calculating state value #
gamma = 0.9

# Control whether env renders # 
render = True

# Device ADDED
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

### Class that stores trajectory data ###

class Step():
    def __init__(self,state,action,reward,next_state):
        self.state = state
        self.action = action
        self.reward = reward
        self.next_state = next_state

### Generating the neural networks ###

# The Critic #

class Critic(nn.Module):
    def __init__(self):
        super(Critic, self).__init__()
        self.layer1 = nn.Linear(4,64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,1)
    
    def forward(self,x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = self.layer3(x)
        return x
     
# The Actor #
        
class Actor(nn.Module):
    def __init__(self):
        super(Actor, self).__init__()
        self.layer1 = nn.Linear(3,64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,1)
        
    def forward(self,x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.tanh(self.layer3(x))
        return x

### Initializing the networks ###

# Initialize actor and critic # 
critic = Critic()
actor = Actor()

# Initialize target actor and target critic with the same parameters as actor and critic #
target_critic = copy.deepcopy(critic)
target_actor = copy.deepcopy(actor)

# Initialize the ADAM optimizers for actor and critic #
critic_optimizer = torch.optim.Adam(critic.parameters(), lr = critic_learning_rate)
actor_optimizer = torch.optim.Adam(actor.parameters(), lr = actor_learning_rate)

### Store the undiscounted return ###
scores = []

### The DDPG ###

# Store initial time #
t = time.time()

# Initialize environment #
env = gym.make('Pendulum-v1')

# Initialize the memory #
episode = []
episode_test = []

# For each episode: #
for n in range(M):
    
    # Initialize undiscounted return at 0 #
    total_reward = 0
    
    # Initialize starting state as a Tensor #
    state = env.reset()
    # state = torch.from_numpy(state).float()
    ## tensor([0.9987, 0.0518, 0.9848])
    # print("state",state)
    # While current state isn't terminal: #
    
    terminal = False
    while not terminal:
        
        # Have the actor network generate an action, then add random noise #
        tensorState = torch.from_numpy(state).float()
        #print(f"Start {time.time() - t}")
        action = actor(tensorState) * 2 
        # print("action",action) 
        action = action.item()
        action += (noise * np.random.randn())
        print("ACTION", action)
        # Take the action generated by the network #
        
        next_state, reward, terminal, _ = env.step([action])
        # next_state = torch.from_numpy(next_state).float()
        # next_state = next_state.astype('object')
        # Record the returns #
        
        total_reward += reward
        
        # Store the transition as a Step object in the memory #
        
        transition = Step(state, action, reward, next_state)
        # print("transition", transition)
        episode_test.append([state,action,reward,next_state])
        print("ST NS", state, "NSNS", next_state)
        episode.append(transition)
        print("ep", episode)
        print("eptest", episode_test)

        episode_array = np.array(episode_test)
        # episode_array = np.array(episode_test, dtype=object)
        print("np eptest", episode_array)
        ## [tensor([ 0.1239,  0.9923, -0.9833]) -0.030708714303215878
        ## -2.1894155583970654
        ## array([ 0.13593362,  0.99071795, -0.2436925 ], dtype=float32)]


        # Render the environment
        if render:
            env.render()
        
        # If the memory can produce a batch, do so #
        
        #print(f"Before batch {time.time() - t}")
        
        if batch_size < len(episode):
            
            # Select random samples from the memory #
            
            # batch = np.random.choice(episode,batch_size)
            # batch = np.array(batch)
            # print("batch", batch)
            batchTESTS = episode_array[np.random.choice(episode_array.shape[0], batch_size, replace=False)]
            print("testbatch", batchTESTS)
            print("DONE batch")
            # print("batchtest", np.random.choice(episode_test,batch_size))
            # Assemble the batch data into a matrix #
            
            # batch_matrix = []
            
            # for step in batch:
            #     temp_list = []
            #     temp_list.append(step.state)
            #     temp_list.append(step.action)
            #     temp_list.append(step.reward)
            #     temp_list.append(step.next_state)
            #     batch_matrix.append(temp_list)
            # print("BATCHM",batch_matrix)

            # batch_matrix = np.array(batch_matrix)
            
            #print(f"After batch {time.time() - t}")
            
            # Arrange the matrix into Tensors of each individual attribute #
            
            # states = batch_matrix[:,0]
            statesTEST = batchTESTS[:,0]
            TENSORstates = torch.Tensor((batchTESTS[:,0]).tolist())
            # TENSORstates = torch.from_numpy(batchTESTS[:,0])
            # actionsTEST = torch.from_numpy(batchTESTS[:,1])
            actionsTEST = batchTESTS[:,1]
            # actions = torch.from_numpy(np.array(batch_matrix[:,1],dtype = np.float16))
            TENSORactions = torch.Tensor((batchTESTS[:,1]).tolist())
            print("TESNORACTIONS", TENSORactions)
            # rewardsTEST = batchTESTS[:,2]
            rewardsTEST = torch.Tensor((batchTESTS[:,2]).tolist())
            # nextStatesTEST = batchTESTS[:,3]
            # nextStatesTEST = torch.from_numpy(batchTESTS[:,3]).float()

            ## extremely slow, consider converting list to single numpy.ndarray w numpy.array() before converting to tensor
            nextStatesTEST = torch.Tensor((batchTESTS[:,3]).tolist())
            # print("STATES", states)
            print("STATESTEST", statesTEST)
            print("ACTIONSTEST", actionsTEST)
            print("REWARDSTEST", rewardsTEST)
            print("NEXTSTATESTEST", nextStatesTEST)
            
            # actions = torch.from_numpy(np.array(batch_matrix[:,1],dtype = np.float16))

            # rewards = torch.from_numpy(np.array(batch_matrix[:,2],dtype = np.float16))

            # next_states = torch.Tensor((batch_matrix[:,3]).tolist())
            # print("nextSTATESOG", next_states)
            # Input the next_state data into the target actor network to get the action generated by the policy #

            # target_policy_output = target_actor(next_states)
            target_policy_output = target_actor(nextStatesTEST)
            print("TPO", target_policy_output)
            # Generate the state value using the target critic network #
            
            # target_critic_input = torch.hstack([next_states,target_policy_output])
            target_critic_input = torch.hstack([nextStatesTEST,target_policy_output])
            # y = rewards + (gamma * torch.flatten(target_critic(target_critic_input)))
            y = rewardsTEST + (gamma * torch.flatten(target_critic(target_critic_input)))
            # Optimize the target critic network #
            print("ACTIONS!", actionsTEST)

            # dif between q prime and actual q values. mse of that
            # need help here: why did we do this the opposite way than above
            critic_optimizer.zero_grad()
            # state_tensor = torch.Tensor(statesTEST)
            print("TSTS", TENSORstates)
            print("TATA", TENSORactions)
            actionReshape = torch.reshape(TENSORactions, (batch_size,1))
            critic_input = torch.hstack([TENSORstates,actionReshape])

            
            # why doesnt this work like above
            critic_output = critic(critic_input)

            # Loss function is the MSE of the state value and the critic network output #
            # cant optimisde before the loss function!
            loss = (y  - torch.flatten(critic(critic_input))) ** 2
            print("lost", loss)
            # Perform gradient descent #
            
            loss.backward(retain_graph=True)
            critic_optimizer.step()
            
            #print(f"After opt1 {time.time() - t}")
            
            # Optimize the target actor network #
            
            qvalues = []
            
            for i in range(batch_size):
                actor_optimizer.zero_grad()
                state_tensor = torch.Tensor(states[i])
                critic_input = torch.hstack([state_tensor,actor(state_tensor)])
                critic_output = critic(critic_input)
                qvalues.append(critic_output)
            
            qvalues = torch.cat(qvalues,0)
            
            # Calculate the expected state value # 
            
            loss = -qvalues.mean()

            # Maximise the expected state value #
            
            loss.backward(retain_graph=True)
            actor_optimizer.step()
            
            #print(f"After opt2 {time.time() - t}")
            
            # Perform an update step on the parameters of the actor and critic networks #
            
            with torch.no_grad():
                for index,parameter in enumerate(target_critic.parameters()):
                    critic_parameter = list(critic.parameters())[index]
                    new_value = (parameter * (1-critic_learning_rate)) + (critic_parameter * critic_learning_rate)
                    parameter.copy_(new_value)
            
            with torch.no_grad():
                for index,parameter in enumerate(target_actor.parameters()):
                    actor_parameter = list(actor.parameters())[index]
                    new_value = (parameter * (1-actor_learning_rate)) + (actor_parameter * actor_learning_rate)
                    parameter.copy_(new_value)
            
        # Update the current state #
            
        # state = torch.from_numpy(next_state).float()
        state = next_state
        print("state and ns", state, "NS", next_state)
        # state = next_state
    # Print the outcome of the episode #
    
    print(f"Episode {n+1}: Total reward: {round(total_reward,1)}, Time elapsed: {round(time.time() - t,2)}s")
    
    # Record the final undiscounted return #
    
    scores.append(total_reward)




