# Imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim
import numpy as np
import gym
from torch.autograd import Variable
import random
import copy
import time
from matplotlib import pyplot as plt 

# DDPG Agent #
class DDPGAgent():
    def __init__(self,environment,episodes):
        
        self.env = gym.make(environment)

        self.episodes = episodes
        self.batch_size = 150
        # magnitude of action noise
        self.noise = 0.05
        # discount for calculating state value
        self.gamma = 0.9
        # control rendering of environment
        self.render = False

        # FOR PENDULUM
        #"""
        if environment == 'Pendulum-v1':
            
            self.action_space = 1
            self.state_space = 3
            self.bounds = 2
        #"""

        #FOR BIPEDAL
        if environment == 'BipedalWalker-v3':

            self.action_space = 4
            self.state_space = 24
            self.bounds = 1


        self.memory = []

        # Initialize actor and critic # 
        self.critic = Critic(self.state_space,self.action_space)
        self.critic2 = Critic(self.state_space,self.action_space)
        self.actor = Actor(self.state_space,self.action_space,self.bounds)

        # Initialize target actor and target critic with the same parameters as actor and critic #
        self.target_critic = copy.deepcopy(self.critic)
        self.target_critic2 = copy.deepcopy(self.critic2)
        self.target_actor = copy.deepcopy(self.actor)
        
        # Initialize the ADAM optimizers for actor and critic #
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = critic_learning_rate)
        self.critic2_optimizer = torch.optim.Adam(self.critic2.parameters(), lr = critic_learning_rate)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = actor_learning_rate)
    
    def generateEpisode(self):

        # Initialize undiscounted return at 0
        total_reward = 0
        
        # Initialize starting state
        state = self.env.reset()

        step = 0

        terminal = False
        while not terminal:
            
            # Have the actor network generate an action, then add random noise
            tensorState = torch.from_numpy(state).float()
            action = self.actor(tensorState) 
            action = torch.add(action,torch.from_numpy(np.random.normal(0,self.noise,self.action_space))).detach().numpy()

            #action = action.item()
            # Take the action generated by the network
            
            if self.action_space == 1:
                action = action.item()
                next_state, reward, terminal, _ = self.env.step([action])
            else:
                next_state, reward, terminal, _ = self.env.step(action)
            
            # Record the returns #
            total_reward += reward

            # Store the transition in memory
            self.memory.append([state,action,reward,next_state])
            
            # Render the environment
            if self.render:
                self.env.render()

            # If the memory can produce a batch
            if self.batch_size < len(self.memory):
                
                # Select random samples from the memory #
                batchIndices = random.sample(range(len(self.memory)),150)
                
                batch_matrix = []

                for index in batchIndices:
                    batch_matrix.append(self.memory[index])

                batch_matrix = np.array(batch_matrix)

                # Arrange the matrix into each individual attribute 
                states = batch_matrix[:,0]
                tensor_states = torch.Tensor((batch_matrix[:,0]).tolist())
                
                if self.action_space == 1:
                    actions = torch.from_numpy(np.array(batch_matrix[:,1],dtype = np.float16))
                    actions = torch.reshape(actions, (self.batch_size,1))
                else:
                    actions = torch.Tensor((batch_matrix[:,1]).tolist())
        
                rewards = torch.from_numpy(np.array(batch_matrix[:,2],dtype = np.float16))
                next_states = torch.Tensor((batch_matrix[:,3]).tolist())

                # Input the next_state data into the target actor network to get the action generated by the policy 
                critic_output = self.critic(tensor_states,actions)
                critic2_output = self.critic2(tensor_states,actions)
                target_policy_output = self.target_actor(next_states)

                target1 = self.target_critic(next_states,target_policy_output)
                target2 = self.target_critic2(next_states,target_policy_output)
                target = torch.max(target1,target2)

                # QPrime
                y = rewards + (self.gamma * torch.flatten(target))       
                y = torch.reshape(y, (self.batch_size,1))
                
                critic_loss = loss(critic_output,y)
                critic2_loss = loss(critic2_output,y)

                actor_output = self.actor(tensor_states)
                critic_output = self.critic(tensor_states,actor_output)
                actor_loss = -torch.mean(critic_output)

                if step % 2 == 0:
                    self.actor_optimizer.zero_grad()
                    actor_loss.backward(retain_graph=True)
                    self.actor_optimizer.step()

                # Maximise the expected state value #
                self.critic_optimizer.zero_grad()
                critic_loss.backward(retain_graph=True)
                self.critic_optimizer.step()

                self.critic2_optimizer.zero_grad()
                critic2_loss.backward(retain_graph=True)
                self.critic2_optimizer.step()

                # Perform an update step on the parameters of the actor and critic networks #
                with torch.no_grad():
                    for target_parameter,parameter in zip(self.target_critic.parameters(),self.critic.parameters()):
                        target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))
                
                with torch.no_grad():
                    for target_parameter,parameter in zip(self.target_critic2.parameters(),self.critic2.parameters()):
                        target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))
                
                if step % 2 == 0:
                    with torch.no_grad():
                        for target_parameter,parameter in zip(self.target_actor.parameters(),self.actor.parameters()):
                            target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))
            # Update the current state 
            state = next_state
            step += 1

        return total_reward

# The Critic #
class Critic(nn.Module):
    def __init__(self,state,action):
        super(Critic, self).__init__()
        self.layer1 = nn.Linear((state+action),64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,1)
    
    def forward(self,states,actions):
        x = torch.cat([states,actions],1)
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = self.layer3(x)
        return x
     
# The Actor #       
class Actor(nn.Module):
    def __init__(self,state,action,bounds):
        super(Actor, self).__init__()
        self.bounds = bounds
        self.layer1 = nn.Linear(state,64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,action)
        
    def forward(self,x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.tanh(self.layer3(x))
        return x * self.bounds

## Actual Run ##

# Optimizer learning rates 
critic_learning_rate = 0.01
actor_learning_rate = 0.001

loss = nn.MSELoss()
all_scores = []

t = time.time()
    # For each agent
for a in range(20):
    agent = DDPGAgent('Pendulum-v1',50)
    #agent = DDPGAgent('BipedalWalker-v3',1500)
    agentRewards = []

    ### Store the undiscounted return ###
    scores = []

    for e in range(agent.episodes):
        episodeReward = agent.generateEpisode()
        agentRewards.append(episodeReward)
        print(f"Episode {e+1}: Total reward: {episodeReward}, Time elapsed: {(time.time() - t)}s")
    print(f"{a+1} Done!")    
    all_scores.append(agentRewards)

all_scores = np.array(all_scores)
mean_score = all_scores.mean(0)
plt.plot(mean_score)
plt.show()
