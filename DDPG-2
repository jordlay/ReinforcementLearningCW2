### Importing all the required python modules ###

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim
import numpy as np
import gym
from torch.autograd import Variable
import copy
import time
from matplotlib import pyplot as plt 
# from utils import *
# from model import *

### Setting hyperparameters ###

#Number of episodes
M = 50

# Optimizer learning rates #
critic_learning_rate = 0.005
actor_learning_rate = 0.0005

# Magnitude of action noise #
noise = 0.05

# Size of batch #
batch_size = 150

# Discount for calculating state value #
gamma = 0.9

# Control whether env renders # 
render = False

### Generating the neural networks ###

# The Critic #

class Critic(nn.Module):
    def __init__(self):
        super(Critic, self).__init__()
        self.layer1 = nn.Linear(4,64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,1)
    
    def forward(self,states,actions):
        x = torch.cat([states,actions],1)
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = self.layer3(x)
        return x
     
# The Actor #
        
class Actor(nn.Module):
    def __init__(self):
        super(Actor, self).__init__()
        self.layer1 = nn.Linear(3,64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,1)
        
    def forward(self,x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.tanh(self.layer3(x))
        return x*2

### Initializing the networks ###

# Initialize actor and critic # 

critic = Critic()
actor = Actor()

# Initialize target actor and target critic with the same parameters as actor and critic #

target_critic = copy.deepcopy(critic)
target_actor = copy.deepcopy(actor)

# Initialize the ADAM optimizers for actor and critic #

critic_optimizer = torch.optim.Adam(critic.parameters(), lr = critic_learning_rate)
actor_optimizer = torch.optim.Adam(actor.parameters(), lr = actor_learning_rate)

### Store the undiscounted return ###

scores = []

### The DDPG ###

# Store initial time #

t = time.time()

# Initialize environment #

env = gym.make('Pendulum-v1')

# Initialize the memory #

# episode = []
memory = []
# For each episode: #

for n in range(M):
    
    # Initialize undiscounted return at 0 #
    
    total_reward = 0
    
    # Initialize starting state as a Tensor #
    
    state = env.reset()
    # state = torch.from_numpy(state).float()
    # print("state",state)
    # While current state isn't terminal: #
   
    terminal = False
    while not terminal:
        
        # Have the actor network generate an action, then add random noise #
        
        tensorState = torch.from_numpy(state).float()
        action = actor(tensorState) 
        # print("action",action)
        action = action.item()
        action += (noise * np.random.randn())
        # print("ACTION", action)
        # Take the action generated by the network #
        
        next_state, reward, terminal, _ = env.step([action])
        
        # Record the returns #
        
        total_reward += reward
        
        # Store the transition as a Step object in the memory #
        memory.append([state,action,reward,next_state])
        memoryArray = np.array(memory)
        # print("mem", memory)
        # Render the environment
        
        if render:
            env.render()
        
        # If the memory can produce a batch, do so #
        
        #print(f"Before batch {time.time() - t}")
        
        if batch_size < len(memoryArray):
            
            # Select random samples from the memory #
        
            
            batch_matrix = memoryArray[np.random.choice(memoryArray.shape[0], batch_size, replace=False)]
        
            #print(f"After batch {time.time() - t}")
            
            # Arrange the matrix into Tensors of each individual attribute #
            
            states = batch_matrix[:,0]
            tensor_states = torch.Tensor((batch_matrix[:,0]).tolist())
            # print("states", states)
            actions = torch.from_numpy(np.array(batch_matrix[:,1],dtype = np.float16))
            actions = torch.reshape(actions, (batch_size,1))
            rewards = torch.from_numpy(np.array(batch_matrix[:,2],dtype = np.float16))
            # print("REWARDS", rewards)
            next_states = torch.Tensor((batch_matrix[:,3]).tolist())
            # print("nextstates", next_states)

            # Input the next_state data into the target actor network to get the action generated by the policy #
            critic_output = critic(tensor_states,actions)
            target_policy_output = target_actor(next_states)
            #QPrime
            y = rewards + (gamma * torch.flatten(target_critic(next_states,target_policy_output)))
                      
            y = torch.reshape(y, (batch_size,1))
            # print("y", y.shape)  
            # Loss function is the MSE of the state value and the critic network output #
            loss = nn.MSELoss()
            critic_loss = loss(critic_output,y)
            # loss = (y - critic_output) ** 2
            # print("cl", critic_loss)
            # Perform gradient descent #

        
            #print(f"After opt1 {time.time() - t}")
            # torch.autograd.set_detect_anomaly(True)
            # Optimize the target actor network #
            actor_output = actor(tensor_states)
            critic_outout = critic(tensor_states,actor_output)
            #print("CO", critic_output)
            actor_loss = -torch.mean(critic_output)
            print("actor loss", actor_loss)

            actor_optimizer.zero_grad()
            actor_loss.backward(retain_graph=True)
            print("AL after", actor_loss)
            actor_optimizer.step()
            # Maximise the expected state value #
            critic_optimizer.zero_grad()
            critic_loss.backward(retain_graph=True)
            critic_optimizer.step()

        
            # torch.autograd.set_detect_anomaly(True)
            #print(f"After opt2 {time.time() - t}")
            
            # Perform an update step on the parameters of the actor and critic networks #
            print("before", list(target_actor.parameters())[0][0])
            with torch.no_grad():
                for index,parameter in enumerate(target_actor.parameters()):
                    actor_parameter = list(actor.parameters())[index]
                    new_value = (parameter * (1-actor_learning_rate)) + (actor_parameter * actor_learning_rate)
                    parameter.copy_(new_value)
            
            with torch.no_grad():
                for index,parameter in enumerate(target_critic.parameters()):
                    
                    critic_parameter = list(critic.parameters())[index]
                    new_value = (parameter * (1-critic_learning_rate)) + (critic_parameter * critic_learning_rate)
                    parameter.copy_(new_value)
            
            
            print("aftyer", list(target_actor.parameters())[0][0])
        # Update the current state #
            

        state = next_state
    
    # Print the outcome of the episode #
    
    print(f"Episode {n+1}: Total reward: {round(total_reward,1)}, Time elapsed: {round(time.time() - t,2)}s")
    
    # Record the final undiscounted return #
    
    scores.append(total_reward)

plt.plot(scores)
plt.show()
