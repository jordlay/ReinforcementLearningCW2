### Importing all the required python modules ###

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim
import numpy as np
import gym
from torch.autograd import Variable
import random
import copy
import time
from matplotlib import pyplot as plt 
# from utils import *
# from model import *

### Setting hyperparameters ###

loss = nn.MSELoss()

#Number of episodes
M = 50

# Optimizer learning rates #
critic_learning_rate = 0.01
actor_learning_rate = 0.001

# Magnitude of action noise #
noise = 0.05

# Size of batch #
batch_size = 150

# Discount for calculating state value #
gamma = 0.9

# Control whether env renders # 
render = False

### Generating the neural networks ###

# The Critic #

class Critic(nn.Module):
    def __init__(self):
        super(Critic, self).__init__()
        self.layer1 = nn.Linear(4,64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,1)
    
    def forward(self,states,actions):
        x = torch.cat([states,actions],1)
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = self.layer3(x)
        return x
     
# The Actor #
        
class Actor(nn.Module):
    def __init__(self):
        super(Actor, self).__init__()
        self.layer1 = nn.Linear(3,64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,1)
        
    def forward(self,x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.tanh(self.layer3(x))
        return x*2
all_scores = []
### Initializing the networks ###
for l in range(20):
    # Initialize actor and critic # 

    critic = Critic()
    actor = Actor()

    # Initialize target actor and target critic with the same parameters as actor and critic #

    target_critic = copy.deepcopy(critic)
    target_actor = copy.deepcopy(actor)

    # Initialize the ADAM optimizers for actor and critic #

    critic_optimizer = torch.optim.Adam(critic.parameters(), lr = critic_learning_rate)
    actor_optimizer = torch.optim.Adam(actor.parameters(), lr = actor_learning_rate)

    ### Store the undiscounted return ###

    scores = []

    ### The DDPG ###

    # Store initial time #

    t = time.time()

    # Initialize environment #

    env = gym.make('Pendulum-v0')

    # Initialize the memory #

    # episode = []
    memory = []
    # For each episode: #

    for n in range(M):
        
        # Initialize undiscounted return at 0 #
        
        total_reward = 0
        
        # Initialize starting state as a Tensor #
        
        state = env.reset()
        # state = torch.from_numpy(state).float()
        # print("state",state)
        # While current state isn't terminal: #
    
        terminal = False
        while not terminal:
            
            # Have the actor network generate an action, then add random noise #
            
            tensorState = torch.from_numpy(state).float()
            action = actor(tensorState) 
            # print("action",action)
            #print(f"State: {state} \nAction: {action}")
            action = action.item()
            action += (noise * np.random.randn())
            # print("ACTION", action)
            # Take the action generated by the network #
            
            next_state, reward, terminal, _ = env.step([action])
            
            # Record the returns #
            
            total_reward += reward
            
            # Store the transition as a Step object in the memory #
            memory.append([state,action,reward,next_state])
            #memoryArray = np.array(memory)
            # print("mem", memory)
            # Render the environment
            
            if render:
                env.render()
            
            # If the memory can produce a batch, do so #
            
            #print(f"Before batch {time.time() - t}")
            
            if batch_size < len(memory):
                
                # Select random samples from the memory #
                batchIndices = random.sample(range(len(memory)),150)
                
                batch_matrix = []

                for index in batchIndices:
                    batch_matrix.append(memory[index])

                batch_matrix = np.array(batch_matrix)
                #print(f"After batch {time.time() - t}")
                
                # Arrange the matrix into Tensors of each individual attribute #
                
                states = batch_matrix[:,0]
                tensor_states = torch.Tensor((batch_matrix[:,0]).tolist())
                # print("states", states)
                actions = torch.from_numpy(np.array(batch_matrix[:,1],dtype = np.float16))
                actions = torch.reshape(actions, (batch_size,1))
                rewards = torch.from_numpy(np.array(batch_matrix[:,2],dtype = np.float16))
                # print("REWARDS", rewards)
                next_states = torch.Tensor((batch_matrix[:,3]).tolist())
                # print("nextstates", next_states)

                # Input the next_state data into the target actor network to get the action generated by the policy #
                critic_output = critic(tensor_states,actions)
                target_policy_output = target_actor(next_states)
                #QPrime
                y = rewards + (gamma * torch.flatten(target_critic(next_states,target_policy_output)))
                        
                y = torch.reshape(y, (batch_size,1))
                # print("y", y.shape)  
                # Loss function is the MSE of the state value and the critic network output #
                
                critic_loss = loss(critic_output,y)
                # loss = (y - critic_output) ** 2
                #print("cl", critic_loss)
                # Perform gradient descent #

            
                #print(f"After opt1 {time.time() - t}")
                # torch.autograd.set_detect_anomaly(True)
                # Optimize the target actor network #
                actor_output = actor(tensor_states)
                critic_output = critic(tensor_states,actor_output)
                #print("CO", critic_output)
                actor_loss = -torch.mean(critic_output)
                #print("actor loss", actor_loss)

                #print("before", list(actor.parameters())[0][0][0])

                actor_optimizer.zero_grad()
                actor_loss.backward(retain_graph=True)
                actor_optimizer.step()

                #print("after", list(actor.parameters())[0][0][0])
                #print("before", list(critic.parameters())[0][0])

                # Maximise the expected state value #
                critic_optimizer.zero_grad()
                critic_loss.backward(retain_graph=True)
                critic_optimizer.step()

                #print(list(critic.parameters()))

                #print("after", list(critic.parameters())[0][0])

                # torch.autograd.set_detect_anomaly(True)
                #print(f"After opt2 {time.time() - t}")
                
                # Perform an update step on the parameters of the actor and critic networks #
                #print("before", list(target_actor.parameters())[0][0])
                with torch.no_grad():
                    for target_parameter,parameter in zip(target_critic.parameters(),critic.parameters()):
                        target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))
                
                with torch.no_grad():
                    for target_parameter,parameter in zip(target_actor.parameters(),actor.parameters()):
                        target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))
                
                
                #print("after", list(target_actor.parameters())[0][0])
            # Update the current state #
                

            state = next_state
        
        # Print the outcome of the episode #
        
        print(f"Episode {n+1}: Total reward: {round(total_reward,1)}, Time elapsed: {round(time.time() - t,2)}s")
        
        # Record the final undiscounted return #
        
        scores.append(total_reward)
    print(f"{l} Done!")
    all_scores.append(scores)
all_scores = np.array(all_scores)
mean_score = all_scores.mean(0)
plt.plot(mean_score)
plt.show()
