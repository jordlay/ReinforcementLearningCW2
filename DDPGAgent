# Imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim
import numpy as np
import gym
from torch.autograd import Variable
import random
import copy
import time
from matplotlib import pyplot as plt 

# DDPG Agent #
class DDPGAgent():
    def __init__(self):
        self.episodes = 50
        self.batch_size = 150
        # magnitude of action noise
        self.noise = 0.5
        # discount for calculating state value
        self.gamma = 0.9
        # control rendering of environment
        self.render = False
    
    def generateEpisode(self):

        # Initialize undiscounted return at 0
        total_reward = 0
        
        # Initialize starting state
        state = env.reset()

        terminal = False
        while not terminal:
            
            # Have the actor network generate an action, then add random noise
            tensorState = torch.from_numpy(state).float()
            action = actor(tensorState) 
            action = action.item()
            action += (agent.noise * np.random.randn())

            # Take the action generated by the network
            next_state, reward, terminal, _ = env.step([action])
            
            # Record the returns #
            total_reward += reward

            # Store the transition in memory
            memory.append([state,action,reward,next_state])
            
            # Render the environment
            if agent.render:
                env.render()

            # If the memory can produce a batch
            if agent.batch_size < len(memory):
                
                # Select random samples from the memory #
                batchIndices = random.sample(range(len(memory)),150)
                
                batch_matrix = []

                for index in batchIndices:
                    batch_matrix.append(memory[index])

                batch_matrix = np.array(batch_matrix)

                # Arrange the matrix into each individual attribute 
                states = batch_matrix[:,0]
                tensor_states = torch.Tensor((batch_matrix[:,0]).tolist())
                actions = torch.from_numpy(np.array(batch_matrix[:,1],dtype = np.float16))
                actions = torch.reshape(actions, (agent.batch_size,1))
                rewards = torch.from_numpy(np.array(batch_matrix[:,2],dtype = np.float16))
                next_states = torch.Tensor((batch_matrix[:,3]).tolist())

                # Input the next_state data into the target actor network to get the action generated by the policy 
                critic_output = critic(tensor_states,actions)
                target_policy_output = target_actor(next_states)

                # QPrime
                y = rewards + (agent.gamma * torch.flatten(target_critic(next_states,target_policy_output)))       
                y = torch.reshape(y, (agent.batch_size,1))
                
                critic_loss = loss(critic_output,y)
                actor_output = actor(tensor_states)
                critic_output = critic(tensor_states,actor_output)
                actor_loss = -torch.mean(critic_output)

                actor_optimizer.zero_grad()
                actor_loss.backward(retain_graph=True)
                actor_optimizer.step()

                # Maximise the expected state value #
                critic_optimizer.zero_grad()
                critic_loss.backward(retain_graph=True)
                critic_optimizer.step()

                # Perform an update step on the parameters of the actor and critic networks #
                with torch.no_grad():
                    for target_parameter,parameter in zip(target_critic.parameters(),critic.parameters()):
                        target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))
                
                with torch.no_grad():
                    for target_parameter,parameter in zip(target_actor.parameters(),actor.parameters()):
                        target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))
            # Update the current state 
            state = next_state

        return total_reward

# The Critic #
class Critic(nn.Module):
    def __init__(self):
        super(Critic, self).__init__()
        self.layer1 = nn.Linear(4,64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,1)
    
    def forward(self,states,actions):
        x = torch.cat([states,actions],1)
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = self.layer3(x)
        return x
     
# The Actor #       
class Actor(nn.Module):
    def __init__(self):
        super(Actor, self).__init__()
        self.layer1 = nn.Linear(3,64)
        self.layer2 = nn.Linear(64,64)
        self.layer3 = nn.Linear(64,1)
        
    def forward(self,x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.tanh(self.layer3(x))
        return x*2

## Actual Run ##

# Optimizer learning rates 
critic_learning_rate = 0.01
actor_learning_rate = 0.001

loss = nn.MSELoss()
all_scores = []
env = gym.make('Pendulum-v1')
t = time.time()
    # For each agent
for a in range(20):
    agent = DDPGAgent()
    agentRewards = []
    
    # Initialize actor and critic # 
    critic = Critic()
    actor = Actor()

    # Initialize target actor and target critic with the same parameters as actor and critic #
    target_critic = copy.deepcopy(critic)
    target_actor = copy.deepcopy(actor)
    
    # Initialize the ADAM optimizers for actor and critic #
    critic_optimizer = torch.optim.Adam(critic.parameters(), lr = critic_learning_rate)
    actor_optimizer = torch.optim.Adam(actor.parameters(), lr = actor_learning_rate)

    ### Store the undiscounted return ###
    scores = []
    memory = []

    for e in range(agent.episodes):
        episodeReward = agent.generateEpisode()
        agentRewards.append(episodeReward)
        print(f"Episode {e+1}: Total reward: {episodeReward}, Time elapsed: {(time.time() - t)}s")
    print(f"{a+1} Done!")    
    all_scores.append(agentRewards)

all_scores = np.array(all_scores)
mean_score = all_scores.mean(0)
plt.plot(mean_score)
plt.show()
