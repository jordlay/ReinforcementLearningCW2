{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fd9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efce3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, scaling, state_size, hidden_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x)) * scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a71d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4f5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Agent():\n",
    "    def __init__(self, scaling, render, buffer_size, batch_size, gamma, tau, noise,\n",
    "                state_size, hidden_size, action_size, actor_lr, critic_lr):\n",
    "        \n",
    "        self.scaling = scaling\n",
    "        self.render = render\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.noise = noise\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.action_size = action_size\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        \n",
    "        self.actor = Actor(scaling, state_size, hidden_size, action_size)\n",
    "        self.critic = Critic(state_size, hidden_size, action_size)\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = critic_lr)\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.next_state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.terminal_memory = []\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "    def train(self):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        num_step = 0\n",
    "        terminal = False\n",
    "        \n",
    "        while not terminal:\n",
    "            \n",
    "            if self.render:\n",
    "                env.render()\n",
    "                \n",
    "            tensor_state = torch.from_numpy(state).float()            \n",
    "            action = self.actor(tensor_state)\n",
    "            action = torch.add(action, torch.from_numpy(np.random.normal(0, self.noise, self.action_size)))\n",
    "            action = action.detach().numpy()\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if self.memory_counter < self.buffer_size: \n",
    "                self.state_memory.append(state)\n",
    "                self.next_state_memory.append(next_state)\n",
    "                self.action_memory.append(action)\n",
    "                self.reward_memory.append(reward)\n",
    "                self.terminal_memory.append(1 - terminal) # terminal = 1 if true so 0 if terminal            \n",
    "            else:     \n",
    "                index = self.memory_counter % self.buffer_size\n",
    "                self.state_memory[index] = state\n",
    "                self.next_state_memory[index] = next_state\n",
    "                self.action_memory[index] = action\n",
    "                self.reward_memory[index] = reward\n",
    "                self.terminal_memory[index] = 1 - terminal\n",
    "            self.memory_counter += 1 \n",
    "                \n",
    "            # If more items in memory list than batch size, sample items from each memory list\n",
    "            if self.memory_counter >= self.batch_size: \n",
    "                batch = random.sample(range(len(self.state_memory)), self.batch_size) \n",
    "                states = []\n",
    "                next_states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                terminals = []\n",
    "                \n",
    "                for item in batch:\n",
    "                    states.append(self.state_memory[item])\n",
    "                    next_states.append(self.next_state_memory[item])\n",
    "                    actions.append(self.action_memory[item])\n",
    "                    rewards.append(self.reward_memory[item])\n",
    "                    terminals.append(self.terminal_memory[item])\n",
    "                \n",
    "                # Reformat sample lists \n",
    "                states = np.array(states)\n",
    "                next_states = np.array(next_states)\n",
    "                actions = np.array(actions)\n",
    "                states = torch.tensor(states)\n",
    "                next_states = torch.tensor(next_states)\n",
    "                actions = torch.tensor(actions)\n",
    "                states = states.to(torch.float32) \n",
    "                actions = actions.to(torch.float32) \n",
    "                next_states = next_states.to(torch.float32) \n",
    "                \n",
    "                # Gradient descent on critic prep\n",
    "                target_actions = self.target_actor(next_states)\n",
    "                next_Qtargets = self.target_critic(next_states, target_actions)\n",
    "                Qtargets = []\n",
    "                for i in range(self.batch_size):\n",
    "                    Qtargets.append(rewards[i] + self.gamma * next_Qtargets[i] * terminals[i]) # If next state terminal, will be 0\n",
    "                Qtargets = torch.tensor(Qtargets)\n",
    "                Qtargets = Qtargets.view(self.batch_size, 1)\n",
    "                \n",
    "                # Gradient descent on critic\n",
    "                Qexpected = self.critic(states, actions)\n",
    "                critic_loss = nn.MSELoss()(Qexpected, Qtargets)\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "                # Gradient ascent on actor\n",
    "                actions_predicted = self.actor(states)\n",
    "                actor_loss = -self.critic(states, actions_predicted)\n",
    "                actor_loss = torch.mean(actor_loss)\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Update target networks\n",
    "                with torch.no_grad():\n",
    "                    for target_parameter, parameter in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "                        target_parameter.copy_((target_parameter * (1 - self.tau)) + (parameter * self.tau))                    \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for target_parameter, parameter in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "                        target_parameter.copy_((target_parameter * (1 - self.tau)) + (parameter * self.tau))   \n",
    "                \n",
    "            state = next_state\n",
    "            num_step += 1\n",
    "            \n",
    "        if self.render:\n",
    "            env.close()\n",
    "            \n",
    "        return ep_reward\n",
    "    \n",
    "    def test(self):\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for i in range(100):\n",
    "            state = env.reset()\n",
    "            terminal = False\n",
    "            ep_reward = 0\n",
    "\n",
    "            while not terminal:\n",
    "                tensor_state = torch.from_numpy(state).float()\n",
    "                action = self.actor(tensor_state)\n",
    "                action = torch.add(action, torch.from_numpy(np.random.normal(0, self.noise, self.action_size)))\n",
    "                action = action.detach().numpy()\n",
    "                next_state, reward, terminal, _ = env.step(action)\n",
    "                ep_reward += reward \n",
    "                state = next_state\n",
    "                \n",
    "            print('Episode reward:', ep_reward)\n",
    "            scores.append(ep_reward)\n",
    "            \n",
    "        print('SCORES DURING TESTING')\n",
    "        x = [i for i in range(len(scores))]\n",
    "        plt.plot(x, scores)\n",
    "        plt.ylabel('SCORE')       \n",
    "        plt.xlabel('EPISODES')\n",
    "        plt.show()\n",
    "\n",
    "    def visualise(self):\n",
    "        for i in range(5):\n",
    "            state = env.reset()\n",
    "            terminal = False\n",
    "            ep_reward = 0\n",
    "\n",
    "            while not terminal:\n",
    "                env.render()\n",
    "                tensor_state = torch.from_numpy(state).float()\n",
    "                action = self.actor(tensor_state)\n",
    "                action = torch.add(action, torch.from_numpy(np.random.normal(0, self.noise, self.action_size)))\n",
    "                action = action.detach().numpy()\n",
    "                next_state, reward, terminal, _ = env.step(action)\n",
    "                ep_reward += reward \n",
    "                state = next_state\n",
    "\n",
    "            print('Episode reward:', ep_reward)\n",
    "            env.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792bd6ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/2000 Score: -116.811 \t Average over last 100 episodes: -116.811\n",
      "Episode: 2/2000 Score: -103.725 \t Average over last 100 episodes: -110.268\n",
      "Episode: 3/2000 Score: -110.422 \t Average over last 100 episodes: -110.319\n",
      "Episode: 4/2000 Score: -105.899 \t Average over last 100 episodes: -109.214\n",
      "Episode: 5/2000 Score: -116.266 \t Average over last 100 episodes: -110.624\n",
      "Episode: 6/2000 Score: -129.419 \t Average over last 100 episodes: -113.757\n",
      "Episode: 7/2000 Score: -146.483 \t Average over last 100 episodes: -118.432\n",
      "Episode: 8/2000 Score: -236.957 \t Average over last 100 episodes: -133.248\n",
      "Episode: 9/2000 Score: -115.274 \t Average over last 100 episodes: -131.250\n",
      "Episode: 10/2000 Score: -111.593 \t Average over last 100 episodes: -129.285\n",
      "Episode: 11/2000 Score: -160.298 \t Average over last 100 episodes: -132.104\n",
      "Episode: 12/2000 Score: -111.712 \t Average over last 100 episodes: -130.405\n",
      "Episode: 13/2000 Score: -108.041 \t Average over last 100 episodes: -128.685\n",
      "Episode: 14/2000 Score: -116.200 \t Average over last 100 episodes: -127.793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m agent_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, episodes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     ep_reward \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     agent_scores\u001b[38;5;241m.\u001b[39mappend(ep_reward)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mDDPG_Agent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m     44\u001b[0m tensor_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()            \n\u001b[0;32m---> 45\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39madd(action, torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size)))\n\u001b[1;32m     47\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)) \u001b[38;5;241m*\u001b[39m scaling\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1442\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "environment = 'BipedalWalker-v3' # Change environment name here\n",
    "env = gym.make(environment)\n",
    "\n",
    "if environment == 'Pendulum-v1':\n",
    "    scaling = 2\n",
    "elif environment == 'LunarLanderContinuous-v2' or 'BipedalWalker-v3' or 'BipedalWalkerHardcore-v3':\n",
    "    scaling = 1\n",
    "\n",
    "agents = 1\n",
    "episodes = 2000\n",
    "all_scores = []\n",
    "\n",
    "for i in range(1, agents + 1):\n",
    "    \n",
    "    agent = DDPG_Agent(scaling = scaling, render = False, buffer_size = 1000000, batch_size = 100, gamma = 0.99, tau = 0.001, noise = 0.05,\n",
    "                state_size = env.observation_space.shape[0], hidden_size = 300, action_size = env.action_space.shape[0], actor_lr = 0.0001, critic_lr = 0.001)\n",
    "    agent_scores = []\n",
    "    \n",
    "    for j in range(1, episodes + 1):\n",
    "        \n",
    "        ep_reward = agent.train()\n",
    "        agent_scores.append(ep_reward)\n",
    "        if j % 100 == 0:\n",
    "            print('Episode: {}/{} Score: {:.3f} \\t Average over last 100 episodes: {:.3f}'.format(j, episodes, ep_reward, np.mean(agent_scores[-100:]))) \n",
    "    \n",
    "    print('*************************************************************')\n",
    "    print('Agent {} complete\\tAverage score of agent: {:.2f}'.format(i, np.mean(agent_scores))) \n",
    "    print('*************************************************************')\n",
    "    all_scores.append(agent_scores)\n",
    "\n",
    "all_scores = np.array(all_scores)\n",
    "mean_score = all_scores.mean(0)\n",
    "x = [i for i in range(len(mean_score))]\n",
    "print('SCORES DURING TRAINING')\n",
    "plt.plot(x, mean_score)\n",
    "plt.ylabel('MEAN SCORE ACROSS AGENTS')       \n",
    "plt.xlabel('EPISODES')              \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b20cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.visualise()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
