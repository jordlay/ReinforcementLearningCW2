{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a69d43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing all the required python modules ###\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import gym\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05acf655",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting hyperparameters ###\n",
    "\n",
    "#Number of episodes\n",
    "M = 1000\n",
    "\n",
    "# Optimizer learning rates #\n",
    "critic_learning_rate = 0.001\n",
    "actor_learning_rate = 0.0001\n",
    "\n",
    "# Magnitude of action noise #\n",
    "noise = 0.05\n",
    "\n",
    "# Size of batch #\n",
    "batch_size = 50\n",
    "\n",
    "# Discount for calculating state value #\n",
    "gamma = 0.9\n",
    "\n",
    "# Control whether env renders # \n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d2a03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class that stores trajectory data ###\n",
    "\n",
    "class Step():\n",
    "    def __init__(self,state,action,reward,next_state):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6955aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating the neural networks ###\n",
    "\n",
    "# The Critic #\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(4,64)\n",
    "        self.layer2 = nn.Linear(64,64)\n",
    "        self.layer3 = nn.Linear(64,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "     \n",
    "# The Actor #\n",
    "        \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(3,64)\n",
    "        self.layer2 = nn.Linear(64,64)\n",
    "        self.layer3 = nn.Linear(64,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.tanh(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07694e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initializing the networks ###\n",
    "\n",
    "# Initialize actor and critic # \n",
    "\n",
    "critic = Critic()\n",
    "actor = Actor()\n",
    "\n",
    "# Initialize target actor and target critic with the same parameters as actor and critic #\n",
    "\n",
    "target_critic = copy.deepcopy(critic)\n",
    "target_actor = copy.deepcopy(actor)\n",
    "\n",
    "# Initialize the ADAM optimizers for actor and critic #\n",
    "\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr = critic_learning_rate)\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr = actor_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f641fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store the undiscounted return ###\n",
    "\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ae0e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:50: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:50: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: -1253.4, Time elapsed: 17.74s\n",
      "Episode 2: -1346.5, Time elapsed: 39.1s\n",
      "Episode 3: -1494.4, Time elapsed: 62.39s\n",
      "Episode 4: -1703.6, Time elapsed: 85.12s\n",
      "Episode 5: -1631.2, Time elapsed: 107.26s\n",
      "Episode 6: -1558.4, Time elapsed: 129.57s\n",
      "Episode 7: -1258.3, Time elapsed: 152.98s\n",
      "Episode 8: -1528.6, Time elapsed: 176.71s\n",
      "Episode 9: -1361.1, Time elapsed: 200.09s\n",
      "Episode 10: -1431.8, Time elapsed: 223.89s\n",
      "Episode 11: -1510.7, Time elapsed: 248.32s\n",
      "Episode 12: -1388.7, Time elapsed: 272.97s\n",
      "Episode 13: -1378.6, Time elapsed: 297.07s\n",
      "Episode 14: -1375.9, Time elapsed: 321.78s\n",
      "Episode 15: -1525.3, Time elapsed: 346.22s\n",
      "Episode 16: -1388.9, Time elapsed: 370.65s\n",
      "Episode 17: -1445.2, Time elapsed: 394.85s\n",
      "Episode 18: -1505.3, Time elapsed: 419.28s\n",
      "Episode 19: -1491.5, Time elapsed: 443.76s\n",
      "Episode 20: -1518.8, Time elapsed: 470.11s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4b93cf86a6fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;31m#print(f\"After opt1 {time.time() - t}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### The DDPG ###\n",
    "\n",
    "# Store initial time #\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# Initialize environment #\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "# Initialize the memory #\n",
    "\n",
    "episode = []\n",
    "\n",
    "# For each episode: #\n",
    "\n",
    "for n in range(M):\n",
    "    \n",
    "    # Initialize undiscounted return at 0 #\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    # Initialize starting state as a Tensor #\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state).float()\n",
    "    \n",
    "    # While current state isn't terminal: #\n",
    "    \n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        \n",
    "        # Have the actor network generate an action, then add random noise #\n",
    "        \n",
    "        #print(f\"Start {time.time() - t}\")\n",
    "        action = actor(state) * 2  \n",
    "        action = action.item()\n",
    "        action += (noise * np.random.randn())\n",
    "\n",
    "        # Take the action generated by the network #\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step([action])\n",
    "        \n",
    "        # Record the returns #\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Store the transition as a Step object in the memory #\n",
    "        \n",
    "        transition = Step(state, action, reward, next_state)\n",
    "        episode.append(transition)\n",
    "        \n",
    "        # Render the environment\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        # If the memory can produce a batch, do so #\n",
    "        \n",
    "        #print(f\"Before batch {time.time() - t}\")\n",
    "        \n",
    "        if batch_size < len(episode):\n",
    "            \n",
    "            # Select random samples from the memory #\n",
    "            \n",
    "            batch = np.random.choice(episode,batch_size)\n",
    "            \n",
    "            # Assemble the batch data into a matrix #\n",
    "            \n",
    "            batch_matrix = []\n",
    "            \n",
    "            for step in batch:\n",
    "                temp_list = []\n",
    "                temp_list.append(step.state)\n",
    "                temp_list.append(step.action)\n",
    "                temp_list.append(step.reward)\n",
    "                temp_list.append(step.next_state)\n",
    "                batch_matrix.append(temp_list)\n",
    "            \n",
    "            batch_matrix = np.array(batch_matrix)\n",
    "            \n",
    "            #print(f\"After batch {time.time() - t}\")\n",
    "            \n",
    "            # Arrange the matrix into Tensors of each individual attribute #\n",
    "            \n",
    "            states = batch_matrix[:,0]\n",
    "            \n",
    "            actions = torch.from_numpy(np.array(batch_matrix[:,1],dtype = np.float16))\n",
    "\n",
    "            rewards = torch.from_numpy(np.array(batch_matrix[:,2],dtype = np.float16))\n",
    "\n",
    "            next_states = torch.Tensor((batch_matrix[:,3]).tolist())\n",
    "            \n",
    "            # Input the next_state data into the target actor network to get the action generated by the policy #\n",
    "\n",
    "            target_policy_output = target_actor(next_states)\n",
    "            \n",
    "            # Generate the state value using the target critic network #\n",
    "            \n",
    "            target_critic_input = torch.hstack([next_states,target_policy_output])\n",
    "\n",
    "            y = rewards + (gamma * torch.flatten(target_critic(target_critic_input)))\n",
    "            \n",
    "            # Optimize the target critic network #\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                critic_optimizer.zero_grad()\n",
    "                state_tensor = torch.Tensor(states[i])\n",
    "                critic_input = torch.hstack([state_tensor,actions[i]])\n",
    "                critic_output = critic(critic_input)\n",
    "\n",
    "                # Loss function is the MSE of the state value and the critic network output #\n",
    "                \n",
    "                loss = (y[i]  - critic_output) ** 2\n",
    "                \n",
    "                # Perform gradient descent #\n",
    "                \n",
    "                loss.backward(retain_graph=True)\n",
    "                critic_optimizer.step()\n",
    "            \n",
    "            #print(f\"After opt1 {time.time() - t}\")\n",
    "            \n",
    "            # Optimize the target actor network #\n",
    "            \n",
    "            qvalues = []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                actor_optimizer.zero_grad()\n",
    "                state_tensor = torch.Tensor(states[i])\n",
    "                critic_input = torch.hstack([state_tensor,actor(state_tensor)])\n",
    "                critic_output = critic(critic_input))\n",
    "                qvalues.append(critic_output)\n",
    "            \n",
    "            qvalues = torch.cat(qvalues,0)\n",
    "            \n",
    "            # Calculate the expected state value # \n",
    "            \n",
    "            loss = -qvalues.mean()\n",
    "\n",
    "            # Maximise the expected state value #\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            #print(f\"After opt2 {time.time() - t}\")\n",
    "            \n",
    "            # Perform an update step on the parameters of the actor and critic networks #\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for index,parameter in enumerate(target_critic.parameters()):\n",
    "                    critic_parameter = list(critic.parameters())[index]\n",
    "                    new_value = (parameter * (1-critic_learning_rate)) + (critic_parameter * critic_learning_rate)\n",
    "                    parameter.copy_(new_value)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for index,parameter in enumerate(target_actor.parameters()):\n",
    "                    actor_parameter = list(actor.parameters())[index]\n",
    "                    new_value = (parameter * (1-actor_learning_rate)) + (actor_parameter * actor_learning_rate)\n",
    "                    parameter.copy_(new_value)\n",
    "            \n",
    "        # Update the current state #\n",
    "            \n",
    "        state = torch.from_numpy(next_state).float()\n",
    "    \n",
    "    # Print the outcome of the episode #\n",
    "    \n",
    "    print(f\"Episode {n+1}: Total reward: {round(total_reward,1)}, Time elapsed: {round(time.time() - t,2)}s\")\n",
    "    \n",
    "    # Record the final undiscounted return #\n",
    "    \n",
    "    scores.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f7c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
