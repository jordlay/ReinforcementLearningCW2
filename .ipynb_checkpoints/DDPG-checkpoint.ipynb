{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a69d43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import gym\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05acf655",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1000\n",
    "critic_learning_rate = 0.1\n",
    "actor_learning_rate = 0.1\n",
    "noise = 0.05\n",
    "batch_size = 50\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d2a03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step():\n",
    "    def __init__(self,state,action,reward,next_state):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6955aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(4,64)\n",
    "        self.layer2 = nn.Linear(64,64)\n",
    "        self.layer3 = nn.Linear(64,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "        \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(3,64)\n",
    "        self.layer2 = nn.Linear(64,64)\n",
    "        self.layer3 = nn.Linear(64,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.tanh(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07694e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic()\n",
    "actor = Actor()\n",
    "\n",
    "target_critic = copy.deepcopy(critic)\n",
    "target_actor = copy.deepcopy(actor)\n",
    "\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr = critic_learning_rate)\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr = actor_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96ae0e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:50: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:50: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: -1346.1186047657757\n",
      "Episode 2: -1411.9254885761488\n",
      "Episode 3: -1654.0849243393898\n",
      "Episode 4: -1482.8070471245276\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-705b6839209a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m-\u001b[0m \u001b[0mcritic_output\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                 \u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "env = gym.make('Pendulum-v0')\n",
    "episode = []\n",
    "\n",
    "for n in range(M):\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state).float()\n",
    "    \n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        \n",
    "        #print(f\"Start {time.time() - t}\")\n",
    "        \n",
    "        action = actor(state) * 2\n",
    "        \n",
    "        #print(action)\n",
    "        \n",
    "        action = action.item()\n",
    "        action += (noise * np.random.randn())\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step([action])\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        transition = Step(state, action, reward, next_state)\n",
    "        episode.append(transition)\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        #print(f\"Before batch {time.time() - t}\")\n",
    "        \n",
    "        if batch_size < len(episode):\n",
    "            \n",
    "            batch = np.random.choice(episode,batch_size)\n",
    "            batch_matrix = []\n",
    "            \n",
    "            for step in batch:\n",
    "                temp_list = []\n",
    "                temp_list.append(step.state)\n",
    "                temp_list.append(step.action)\n",
    "                temp_list.append(step.reward)\n",
    "                temp_list.append(step.next_state)\n",
    "                batch_matrix.append(temp_list)\n",
    "            \n",
    "            #print(f\"After batch {time.time() - t}\")\n",
    "            \n",
    "            batch_matrix = np.array(batch_matrix)\n",
    "            \n",
    "            states = batch_matrix[:,0]\n",
    "            \n",
    "            actions = torch.from_numpy(np.array(batch_matrix[:,1],dtype = np.float16))\n",
    "\n",
    "            rewards = torch.from_numpy(np.array(batch_matrix[:,2],dtype = np.float16))\n",
    "\n",
    "            next_states = torch.Tensor((batch_matrix[:,3]).tolist())\n",
    "\n",
    "            target_policy_output = target_actor(next_states)\n",
    "            \n",
    "            target_critic_input = torch.hstack([next_states,target_policy_output])\n",
    "\n",
    "            y = rewards + (gamma * torch.flatten(target_critic(target_critic_input)))\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                critic_optimizer.zero_grad()\n",
    "                state_tensor = torch.Tensor(states[i])\n",
    "                critic_input = torch.hstack([state_tensor,actions[i]])\n",
    "                critic_output = critic(critic_input)\n",
    "\n",
    "                loss = (y[i]  - critic_output) ** 2\n",
    "\n",
    "                loss.backward(retain_graph=True)\n",
    "                critic_optimizer.step()\n",
    "            \n",
    "            #print(f\"After opt1 {time.time() - t}\")\n",
    "            \n",
    "            #print(f\"Before: {list(actor.parameters())[0][0]}\")\n",
    "            qvalues = []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                actor_optimizer.zero_grad()\n",
    "                state_tensor = torch.Tensor(states[i])\n",
    "                critic_input = torch.hstack([state_tensor,actor(state_tensor)])\n",
    "                #print(critic_input)\n",
    "                critic_output = critic(critic_input)\n",
    "                #print(critic_output)\n",
    "                qvalues.append(critic_output)\n",
    "            \n",
    "            #print(qvalues)\n",
    "            qvalues = torch.cat(qvalues,0)\n",
    "            loss = -qvalues.mean()\n",
    "            #print(f\"Loss: {loss}\")\n",
    "            #print(loss)\n",
    "            loss.backward(retain_graph=True)\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            #print(f\"After: {list(actor.parameters())[0][0]}\")\n",
    "            \n",
    "            #print(f\"After opt2 {time.time() - t}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for index,parameter in enumerate(target_critic.parameters()):\n",
    "                    critic_parameter = list(critic.parameters())[index]\n",
    "                    new_value = (parameter * (1-critic_learning_rate)) + (critic_parameter * critic_learning_rate)\n",
    "                    parameter.copy_(new_value)\n",
    "            \n",
    "            #print(f\"Before: {list(target_actor.parameters())[0][0]}\")\n",
    "            with torch.no_grad():\n",
    "                for index,parameter in enumerate(target_actor.parameters()):\n",
    "                    actor_parameter = list(actor.parameters())[index]\n",
    "                    new_value = (parameter * (1-actor_learning_rate)) + (actor_parameter * actor_learning_rate)\n",
    "                    parameter.copy_(new_value)\n",
    "            #print(f\"After: {list(target_actor.parameters())[0][0]}\")\n",
    "            \n",
    "        state = torch.from_numpy(next_state).float()\n",
    "        \n",
    "    print(f\"Episode {n+1}: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa494d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor(torch.Tensor([1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73b6421f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor(torch.Tensor([2,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae2e9e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor(torch.Tensor([0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe390c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
