{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fd9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efce3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, scaling, state_size, hidden_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x)) * scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a71d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4f5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Agent():\n",
    "    def __init__(self, scaling, render, buffer_size, batch_size, gamma, tau, noise,\n",
    "                state_size, hidden_size, action_size, actor_lr, critic_lr):\n",
    "        \n",
    "        self.scaling = scaling\n",
    "        self.render = render\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.noise = noise\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.action_size = action_size\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        \n",
    "        self.actor = Actor(scaling, state_size, hidden_size, action_size)\n",
    "        self.critic = Critic(state_size, hidden_size, action_size)\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = critic_lr)\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.next_state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.terminal_memory = []\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "    def train(self):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        num_step = 0\n",
    "        terminal = False\n",
    "        \n",
    "        while not terminal:\n",
    "            \n",
    "            if self.render:\n",
    "                env.render()\n",
    "                \n",
    "            tensor_state = torch.from_numpy(state).float()            \n",
    "            action = self.actor(tensor_state)\n",
    "            action = torch.add(action, torch.from_numpy(np.random.normal(0, self.noise, self.action_size)))\n",
    "            action = action.detach().numpy()\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if self.memory_counter < self.buffer_size: \n",
    "                self.state_memory.append(state)\n",
    "                self.next_state_memory.append(next_state)\n",
    "                self.action_memory.append(action)\n",
    "                self.reward_memory.append(reward)\n",
    "                self.terminal_memory.append(1 - terminal) # terminal = 1 if true so 0 if terminal            \n",
    "            else:     \n",
    "                index = self.memory_counter % self.buffer_size\n",
    "                self.state_memory[index] = state\n",
    "                self.next_state_memory[index] = next_state\n",
    "                self.action_memory[index] = action\n",
    "                self.reward_memory[index] = reward\n",
    "                self.terminal_memory[index] = 1 - terminal\n",
    "            self.memory_counter += 1 \n",
    "                \n",
    "            # If more items in memory list than batch size, sample items from each memory list\n",
    "            if self.memory_counter >= self.batch_size: \n",
    "                batch = random.sample(range(len(self.state_memory)), self.batch_size) \n",
    "                states = []\n",
    "                next_states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                terminals = []\n",
    "                \n",
    "                for item in batch:\n",
    "                    states.append(self.state_memory[item])\n",
    "                    next_states.append(self.next_state_memory[item])\n",
    "                    actions.append(self.action_memory[item])\n",
    "                    rewards.append(self.reward_memory[item])\n",
    "                    terminals.append(self.terminal_memory[item])\n",
    "                \n",
    "                # Reformat sample lists \n",
    "                states = np.array(states)\n",
    "                next_states = np.array(next_states)\n",
    "                actions = np.array(actions)\n",
    "                states = torch.tensor(states)\n",
    "                next_states = torch.tensor(next_states)\n",
    "                actions = torch.tensor(actions)\n",
    "                states = states.to(torch.float32) \n",
    "                actions = actions.to(torch.float32) \n",
    "                next_states = next_states.to(torch.float32) \n",
    "                \n",
    "                # Gradient descent on critic prep\n",
    "                target_actions = self.target_actor(next_states)\n",
    "                next_Qtargets = self.target_critic(next_states, target_actions)\n",
    "                Qtargets = []\n",
    "                for i in range(self.batch_size):\n",
    "                    Qtargets.append(rewards[i] + self.gamma * next_Qtargets[i] * terminals[i]) # If next state terminal, will be 0\n",
    "                Qtargets = torch.tensor(Qtargets)\n",
    "                Qtargets = Qtargets.view(self.batch_size, 1)\n",
    "                \n",
    "                # Gradient descent on critic\n",
    "                Qexpected = self.critic(states, actions)\n",
    "                critic_loss = nn.MSELoss()(Qexpected, Qtargets)\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "                # Gradient ascent on actor\n",
    "                actions_predicted = self.actor(states)\n",
    "                actor_loss = -self.critic(states, actions_predicted)\n",
    "                actor_loss = torch.mean(actor_loss)\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Update target networks\n",
    "                with torch.no_grad():\n",
    "                    for target_parameter, parameter in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "                        target_parameter.copy_((target_parameter * (1 - self.tau)) + (parameter * self.tau))                    \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for target_parameter, parameter in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "                        target_parameter.copy_((target_parameter * (1 - self.tau)) + (parameter * self.tau))   \n",
    "                \n",
    "            state = next_state\n",
    "            num_step += 1\n",
    "            \n",
    "        if self.render:\n",
    "            env.close()\n",
    "            \n",
    "        return ep_reward\n",
    "     \n",
    "    def test(self):\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for i in range(100):\n",
    "            state = env.reset()\n",
    "            terminal = False\n",
    "            ep_reward = 0\n",
    "\n",
    "            while not terminal:\n",
    "                tensor_state = torch.from_numpy(state).float()\n",
    "                action = self.actor(tensor_state)\n",
    "                action = action.detach().numpy()\n",
    "                next_state, reward, terminal, _ = env.step(action)\n",
    "                ep_reward += reward \n",
    "                state = next_state\n",
    "                \n",
    "            scores.append(ep_reward)\n",
    "        \n",
    "        print('Scores during testing')\n",
    "        print('Mean score:', np.mean(scores))\n",
    "        x = [i for i in range(1, len(scores)+1)]\n",
    "        plt.plot(x, scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.show()\n",
    "        \n",
    "    def visualise(self):\n",
    "        for i in range(10):\n",
    "            state = env.reset()\n",
    "            terminal = False\n",
    "            ep_reward = 0\n",
    "\n",
    "            while not terminal:\n",
    "                env.render()\n",
    "                tensor_state = torch.from_numpy(state).float()\n",
    "                action = self.actor(tensor_state)\n",
    "                action = action.detach().numpy()\n",
    "                next_state, reward, terminal, _ = env.step(action)\n",
    "                ep_reward += reward \n",
    "                state = next_state\n",
    "\n",
    "            print('Episode reward:', ep_reward)\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792bd6ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************\n",
      "Agent 1\n",
      "Episode: 100 Score: -111.821 \t Mean score over last 100 episodes: -113.628\n",
      "Episode: 200 Score: -104.237 \t Mean score over last 100 episodes: -105.166\n",
      "Episode: 300 Score: -142.321 \t Mean score over last 100 episodes: -122.350\n",
      "Episode: 400 Score: -113.315 \t Mean score over last 100 episodes: -121.379\n",
      "Episode: 500 Score: -118.578 \t Mean score over last 100 episodes: -120.165\n",
      "Episode: 600 Score: -161.554 \t Mean score over last 100 episodes: -140.774\n",
      "Episode: 700 Score: -112.241 \t Mean score over last 100 episodes: -133.613\n",
      "Episode: 800 Score: -123.548 \t Mean score over last 100 episodes: -122.983\n",
      "Episode: 900 Score: -111.057 \t Mean score over last 100 episodes: -120.650\n",
      "Episode: 1000 Score: -100.037 \t Mean score over last 100 episodes: -96.168\n",
      "Episode: 1100 Score: -93.079 \t Mean score over last 100 episodes: -69.886\n",
      "Episode: 1200 Score: -96.392 \t Mean score over last 100 episodes: -62.479\n",
      "Episode: 1300 Score: -142.864 \t Mean score over last 100 episodes: -113.149\n",
      "Episode: 1400 Score: -155.914 \t Mean score over last 100 episodes: -154.637\n",
      "Episode: 1500 Score: -73.119 \t Mean score over last 100 episodes: -48.414\n",
      "Episode: 1600 Score: -115.414 \t Mean score over last 100 episodes: -114.763\n",
      "Episode: 1700 Score: -143.540 \t Mean score over last 100 episodes: -131.820\n",
      "Episode: 1800 Score: 271.727 \t Mean score over last 100 episodes: 31.585\n",
      "Episode: 1900 Score: 268.710 \t Mean score over last 100 episodes: -8.661\n",
      "Episode: 2000 Score: 153.108 \t Mean score over last 100 episodes: 120.413\n",
      "Episode: 2100 Score: 281.914 \t Mean score over last 100 episodes: 39.336\n",
      "Episode: 2200 Score: -32.434 \t Mean score over last 100 episodes: 92.567\n",
      "Episode: 2300 Score: 258.254 \t Mean score over last 100 episodes: 159.114\n",
      "Episode: 2400 Score: -179.564 \t Mean score over last 100 episodes: -53.723\n",
      "Episode: 2500 Score: -127.440 \t Mean score over last 100 episodes: -79.137\n",
      "Episode: 2600 Score: -139.372 \t Mean score over last 100 episodes: -40.456\n",
      "Episode: 2700 Score: -101.162 \t Mean score over last 100 episodes: -125.892\n",
      "Episode: 2800 Score: -113.444 \t Mean score over last 100 episodes: 0.281\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15596/1973244372.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0magent_moving_means\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msolved\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mprevious\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3001\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15596/1850186866.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0mQtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                     \u001b[0mQtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_Qtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mterminals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# If next state terminal, will be 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m                 \u001b[0mQtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mQtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "environment = 'BipedalWalker-v3' # Change environment name here\n",
    "env = gym.make(environment)\n",
    "\n",
    "if environment == 'Pendulum-v1':\n",
    "    scaling = 2\n",
    "    solved = -100\n",
    "elif environment == 'LunarLanderContinuous-v2':\n",
    "    scaling = 1\n",
    "    solved = 200 \n",
    "elif environment =='BipedalWalker-v3' or 'BipedalWalkerHardcore-v3':\n",
    "    scaling = 1\n",
    "    solved = 300 \n",
    "\n",
    "previous = 100\n",
    "print_every = 100\n",
    "agents = 1\n",
    "\n",
    "for i in range(1, agents + 1): \n",
    "    print('*************************************************************')\n",
    "    print('Agent {}'.format(i))\n",
    "\n",
    "    agent = DDPG_Agent(scaling = scaling, render = False, buffer_size = 1000000, batch_size = 100, gamma = 0.99, tau = 0.001, noise = 0.05 * scaling,\n",
    "                state_size = env.observation_space.shape[0], hidden_size = 300, action_size = env.action_space.shape[0], actor_lr = 0.0004, critic_lr = 0.001)\n",
    "    agent_scores = [0]\n",
    "    agent_moving_means = [0]\n",
    "    episode = 1\n",
    "\n",
    "    while (agent_moving_means[-1] < solved or episode <= previous) and episode < 3001:\n",
    "\n",
    "        ep_reward = agent.train()\n",
    "\n",
    "        if episode == 1: \n",
    "            agent_scores[0] = ep_reward\n",
    "            agent_moving_means[0] = ep_reward\n",
    "        else:\n",
    "            agent_scores.append(ep_reward)\n",
    "            agent_moving_means.append(np.mean(agent_scores[-previous:])) \n",
    "\n",
    "        if episode % print_every == 0:\n",
    "            print('Episode: {} Score: {:.3f} \\t Mean score over last {} episodes: {:.3f}'.format(episode, ep_reward, previous, agent_moving_means[-1])) \n",
    "\n",
    "        episode += 1\n",
    "\n",
    "    episode -= 1\n",
    "    print('*********************************')\n",
    "    print('Agent {} complete \\t Mean score of agent: {:.3f}'.format(i, np.mean(agent_scores))) \n",
    "    print('{} finished in {} episodes \\t Mean score over last {} episodes: {:.3f}'.format(environment, episode, previous, agent_moving_means[-1]))\n",
    "    print('*********************************')\n",
    "\n",
    "    x = [i for i in range(1, len(agent_scores) + 1)]\n",
    "    print('Agent {} scores during training'.format(i))\n",
    "    plt.plot(x, agent_scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episodes')              \n",
    "    plt.show()\n",
    "\n",
    "    x = [i for i in range(1, len(agent_moving_means) + 1)]\n",
    "    print('Agent {} mean score of last {} episodes during training'.format(i, previous))\n",
    "    plt.plot(x, agent_moving_means)\n",
    "    plt.ylabel('Mean score of last {} episodes'.format(previous))       \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b20cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8116/670702611.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984c6a5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8116/1794989037.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent.visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d32f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
