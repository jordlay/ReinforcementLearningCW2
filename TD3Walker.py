# Imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim
import numpy as np
import gym
from torch.autograd import Variable
import random
import copy
import time
from matplotlib import pyplot as plt 

# DDPG Agent #
class DDPGAgent():
    def __init__(self):
        self.episodes = 1500
        self.batch_size = 150
        # magnitude of action noise
        self.noise = 0.05
        # discount for calculating state value
        self.gamma = 0.9
        # control rendering of environment
        self.render = False

        self.memory = []

        self.current_pos = 0

        self.num_step = 0

        # Initialize actor and critic # 
        self.critic = Critic()
        self.actor = Actor()
        self.critic2 = Critic()

        # Initialize target actor and target critic with the same parameters as actor and critic #
        self.target_critic = copy.deepcopy(self.critic)
        self.target_actor = copy.deepcopy(self.actor)
        self.target_critic2 = copy.deepcopy(self.critic2)
        
        # Initialize the ADAM optimizers for actor and critic #
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = critic_learning_rate)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = actor_learning_rate)
        self.critic_optimizer2 = torch.optim.Adam(self.critic2.parameters(), lr = critic_learning_rate)
    
    def generateEpisode(self):

        # Initialize undiscounted return at 0
        total_reward = 0
        
        # Initialize starting state
        state = env.reset()

        self.num_step = 0

        terminal = False
        while not terminal:

            # Have the actor network generate an action, then add random noise
            tensorState = torch.from_numpy(state).float()
            action = self.actor(tensorState)
            action = torch.add(action, torch.from_numpy(np.random.normal(0,self.noise,4))).detach().numpy()
            #action = torch.add(action, torch.from_numpy((self.noise) * np.random.rand(4))).detach().numpy()

            # Take the action generated by the network
            next_state, reward, terminal, _ = env.step(action)
            
            # Record the returns #
            total_reward += reward

            if len(self.memory) > 100000:
                self.memory[self.current_pos] = [state,action,reward,next_state]
                if self.current_pos == 99999:
                    self.current_pos = 0
                else:
                    self.current_pos += 1
            else:
                self.memory.append([state,action,reward,next_state])

            # Store the transition in memory
            
            
            # Render the environment
            if self.render:
                env.render()

            # If the memory can produce a batch
            if self.batch_size < len(self.memory):
                
                # Select random samples from the memory #
                batchIndices = random.sample(range(len(self.memory)),150)
                
                batch_matrix = []

                for index in batchIndices:
                    batch_matrix.append(self.memory[index])

                batch_matrix = np.array(batch_matrix)

                # Arrange the matrix into each individual attribute 
                states = batch_matrix[:,0]
                tensor_states = torch.Tensor((batch_matrix[:,0]).tolist())
                actions = torch.Tensor((batch_matrix[:,1]).tolist())
                rewards = torch.from_numpy(np.array(batch_matrix[:,2],dtype = np.float16))
                next_states = torch.Tensor((batch_matrix[:,3]).tolist())

                # Input the next_state data into the target actor network to get the action generated by the policy 
                critic_output = self.critic(tensor_states,actions)
                critic_output2 = self.critic2(tensor_states,actions)

                #critic_min = torch.min(critic_output,critic_output2)

                target_policy_output = self.target_actor(next_states)
                target_policy_output = torch.add(target_policy_output, torch.from_numpy(np.random.normal(0,self.noise,(self.batch_size,4)))).float()

                # target_policy_output2 = self.target_actor(next_states)
                # target_policy_output2 = torch.add(target_policy_output2, torch.from_numpy(np.random.normal(0,self.noise,(self.batch_size,4)))).float()

                a = self.target_critic(next_states,target_policy_output)
                b = self.target_critic2(next_states,target_policy_output)
                c = torch.min(a,b)


                # QPrime
                y = rewards + (self.gamma * torch.flatten(c))       
                y = torch.reshape(y, (self.batch_size,1))

                # y2 = rewards + (self.gamma * torch.flatten(self.target_critic2(next_states,target_policy_output2)))       
                # y2 = torch.reshape(y, (self.batch_size,1))
                
                # if y.mean() > y2.mean():
                #     y = y2

                critic_loss = loss(critic_output,y)
                critic_loss2 = loss(critic_output2,y)

                # Maximise the expected state value #
                self.critic_optimizer.zero_grad()
                critic_loss.backward(retain_graph=True)
                self.critic_optimizer.step()

                self.critic_optimizer2.zero_grad()
                critic_loss2.backward(retain_graph=True)
                self.critic_optimizer2.step()

                if self.num_step%2 == 0:
                    actor_output = self.actor(tensor_states)
                    actor_loss = -torch.mean(self.critic(tensor_states,actor_output))

                    self.actor_optimizer.zero_grad()
                    actor_loss.backward(retain_graph=True)
                    self.actor_optimizer.step()

                # Perform an update step on the parameters of the actor and critic networks #
                with torch.no_grad():
                    for target_parameter,parameter in zip(self.target_critic.parameters(),self.critic.parameters()):
                        target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))
                
                with torch.no_grad():
                    for target_parameter,parameter in zip(self.target_actor.parameters(),self.actor.parameters()):
                        target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))

                with torch.no_grad():
                    for target_parameter,parameter in zip(self.target_critic2.parameters(),self.critic2.parameters()):
                        target_parameter.copy_((target_parameter * (1-critic_learning_rate)) + (parameter * critic_learning_rate))
            # Update the current state 
            state = next_state

            self.num_step += 1

        return total_reward

# The Critic #
class Critic(nn.Module):
    def __init__(self):
        super(Critic, self).__init__()
        self.layer1 = nn.Linear(28,256)
        self.layer2 = nn.Linear(256,256)
        self.layer3 = nn.Linear(256,256)
        self.layer4 = nn.Linear(256,1)
    
    def forward(self,states,actions):
        x = torch.cat([states,actions],1)
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.relu(self.layer3(x))
        x = self.layer4(x)
        return x
     
# The Actor #       
class Actor(nn.Module):
    def __init__(self):
        super(Actor, self).__init__()
        self.layer1 = nn.Linear(24,256)
        self.layer2 = nn.Linear(256,256)
        self.layer3 = nn.Linear(256,256)
        self.layer4 = nn.Linear(256,4)
        
    def forward(self,x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.relu(self.layer3(x))
        x = F.tanh(self.layer4(x))
        return x

## Actual Run ##

# Optimizer learning rates 
critic_learning_rate = 0.001
actor_learning_rate = 0.0001

loss = nn.MSELoss()
all_scores = []
#env = gym.make('Pendulum-v1')
env = gym.make('BipedalWalker-v3')
t = time.time()
 
# For each agent
for a in range(1):
    agent = DDPGAgent()
    agentRewards = []

    ### Store the undiscounted return ###
    scores = []

    for e in range(agent.episodes):
        episodeReward = agent.generateEpisode()
        agentRewards.append(episodeReward)
        print(f"Episode {e+1}: Total reward: {round(episodeReward,2)}, Time elapsed: {round(time.time() - t, 2)}s, Timesteps: {agent.num_step}")
    print(f"{a+1} Done!")    
    all_scores.append(agentRewards)

all_scores = np.array(all_scores)
mean_score = all_scores.mean(0)
plt.plot(mean_score)
plt.show()
